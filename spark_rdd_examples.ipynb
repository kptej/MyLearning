{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kptej/MyLearning/blob/main/spark_rdd_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rffP1dgOrdJt",
        "outputId": "244d8a88-0b12-415d-966c-918663422c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O_0IeQctjl9"
      },
      "source": [
        "**spark RDD**\n",
        "\n",
        "RDD (Resilient Distributed Dataset) is a core building block of PySpark. It is a fault-tolerant, immutable, distributed collection of objects. Immutable means that once you create an RDD, you cannot change it. The data within RDDs is segmented into logical partitions, allowing for distributed computation across multiple nodes within the cluster.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU6kY3Jkr25Z",
        "outputId": "8f222d37-2657-4031-ffa9-831d19f0f273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7fa62271ab90>\n",
            "Spark_RDD_examples\n",
            "<pyspark.conf.SparkConf object at 0x7fa63aede690>\n",
            "<SparkContext master=local[*] appName=Spark_RDD_examples>\n",
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n",
            "[1, 2, 3, 4, 5]\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "#create sparkcontext using spark, and read the rdd list\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Spark_RDD_examples\").getOrCreate()\n",
        "\n",
        "print(spark)\n",
        "print(spark.sparkContext.appName)\n",
        "print(spark.sparkContext.getConf())\n",
        "print(spark.sparkContext)\n",
        "\n",
        "#create rdd\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "#print RDD\n",
        "print(rdd)\n",
        "#collect RDD\n",
        "print(rdd.collect())\n",
        "print(rdd.count())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgz59JfQ2Hpj",
        "outputId": "e92d883e-31aa-439d-d01c-2d811b9f11b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "print(rdd.collect())\n",
        "\n",
        "print(rdd.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CZhSBFVZk6t",
        "outputId": "0549f319-a7d2-4acb-d380-700dcae17d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method SparkContext.emptyRDD of <SparkContext master=local[*] appName=Spark_RDD_examples>>\n"
          ]
        }
      ],
      "source": [
        "#empty Rdd\n",
        "rdd = spark.sparkContext.emptyRDD\n",
        "\n",
        "print(rdd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#rdd Tranformations\n",
        "\n",
        "\n",
        "Apache Spark's Resilient Distributed Datasets (RDDs) support two primary types of operations: Transformations and Actions. Transformations are lazy operations that define a new RDD from an existing one, while Actions trigger the execution of these transformations and return results\n",
        "\n",
        "**RDD Transformations**\n",
        "\n",
        "Transformations are operations that create a new RDD from an existing one. They are evaluated lazily, meaning computation is deferred until an action requires the result. Transformations can be categorized into:\n",
        "\n",
        "**Narrow Transformations:** Each output partition depends on a single input partition (e.g., map, filter).\n",
        "\n",
        "**Wide Transformations:** Output partitions depend on multiple input partitions, often requiring data shuffling across the cluster (e.g., reduceByKey, join)\n",
        "\n",
        "| Transformation                      | Description                                                               |                                                                                                                                  |\n",
        "| ----------------------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `map(func)`                         | Applies a function to each element, returning a new RDD.                  |                                                                                                                                  |\n",
        "| `flatMap(func)`                     | Similar to `map`, but can return multiple output elements for each input. |                                                                                                                                  |\n",
        "| `filter(func)`                      | Returns elements that satisfy the predicate function.                     |                                                                                                                                  |\n",
        "| `distinct()`                        | Removes duplicate elements.                                               |                                                                                                                                  |\n",
        "| `union(otherRDD)`                   | Returns the union of two RDDs.                                            |                                                                                                                                  |\n",
        "| `intersection(otherRDD)`            | Returns the intersection of two RDDs.                                     |                                                                                                                                  |\n",
        "| `subtract(otherRDD)`                | Returns elements present in the first RDD but not in the second.          |                                                                                                                                  |\n",
        "| `cartesian(otherRDD)`               | Returns the Cartesian product of two RDDs.                                |                                                                                                                                  |\n",
        "| `groupByKey()`                      | Groups values with the same key.                                          |                                                                                                                                  |\n",
        "| `reduceByKey(func)`                 | Merges values with the same key using the specified function.             |                                                                                                                                  |\n",
        "| `sortByKey()`                       | Sorts RDD by key.                                                         |                                                                                                                                  |\n",
        "| `join(otherRDD)`                    | Joins two RDDs by key.                                                    |                                                                                                                                  |\n",
        "| `coalesce(numPartitions)`           | Reduces the number of partitions.                                         |                                                                                                                                  |\n",
        "| `repartition(numPartitions)`        | Reshuffles data into a specified number of partitions.                    |                                                                                                                                  |\n",
        "| `pipe(command)`                     | Pipes each partition through an external command.                         |                                                                                                                                  |\n",
        "| `mapPartitions(func)`               | Applies a function to each partition.                                     |                                                                                                                                  |\n",
        "| `sample(withReplacement, fraction)` | Samples a fraction of the data.                                           | ([sparkcodehub.com][1], [LinkedIn][2], [Apache Spark][3], [Stack Overflow][4], [DataFlair][5], [Medium][6], [Stack Overflow][7]) |\n",
        "\n",
        "[1]: https://www.sparkcodehub.com/spark/rdd/transformations?utm_source=chatgpt.com \"Mastering Apache Spark RDD Transformations - SparkCodeHub\"\n",
        "[2]: https://www.linkedin.com/pulse/spark-transformations-actions-lazy-evaluation-mohammad-younus-jameel?utm_source=chatgpt.com \"Spark Transformations, Actions and Lazy Evaluation. - LinkedIn\"\n",
        "[3]: https://spark.apache.org/docs/latest/rdd-programming-guide.html?utm_source=chatgpt.com \"RDD Programming Guide - Spark 3.5.5 Documentation\"\n",
        "[4]: https://stackoverflow.com/questions/45908291/rdd-transformation-and-actions?utm_source=chatgpt.com \"RDD transformation and actions - apache spark - Stack Overflow\"\n",
        "[5]: https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/?utm_source=chatgpt.com \"Spark RDD Operations-Transformation & Action with Example\"\n",
        "[6]: https://medium.com/%40sujathamudadla1213/spark-transformations-and-actions-ff4b576cbef8?utm_source=chatgpt.com \"Spark RDD Transformations and Actions. | by Sujatha Mudadla\"\n",
        "[7]: https://stackoverflow.com/questions/78722890/where-can-i-find-an-exhaustive-list-of-actions-for-spark?utm_source=chatgpt.com \"Where can I find an exhaustive list of actions for spark?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "cv6XbyWmZUxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rdd Tranformations\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Spark_RDD_examples\").getOrCreate()\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "\n",
        "print(rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAfHk07QZIyq",
        "outputId": "8f99421e-21bb-4c5c-bd05-48012975a10b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#map:Applies a function to each element, returning a new RDD.\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create an RDD from a Python list\n",
        "numbers = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Use the map transformation to square each element\n",
        "squared_numbers = numbers.map(lambda x: x ** 2)\n",
        "\n",
        "num = numbers.map(lambda x: (x,1) )\n",
        "\n",
        "print(num.collect())\n",
        "# Collect the results and print\n",
        "print(squared_numbers.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX0QzP7xZqsT",
        "outputId": "98efd026-0d3f-45b6-cd5f-705a5587d635"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
            "[1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flat map\n",
        "# flatMap() in Spark with an RDD ‚Äî it's often used to split elements into multiple parts, like splitting sentences into words.\n",
        "#üîç Key Differences from map()\n",
        "\"\"\" map() would return a list of lists: one list per sentence.\n",
        "flatMap() flattens those lists into a single list of words.\"\"\"\n",
        "\n",
        "# Create an RDD of sentences\n",
        "sentences = spark.sparkContext.parallelize([\n",
        "    \"Apache Spark is fast\",\n",
        "    \"It supports many operations\"\n",
        "])\n",
        "\n",
        "# Use flatMap to split each sentence into words\n",
        "words = sentences.flatMap(lambda sentence: sentence.split())\n",
        "\n",
        "w = sentences.flatMap(lambda x: x.split())\n",
        "\n",
        "print(w.foreach(lambda x : print(x)))\n",
        "\n",
        "# Collect and print the words\n",
        "print(words.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY1n19LFafNP",
        "outputId": "d399bf95-e1fb-4229-8b9d-80bb964fc49b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "['Apache', 'Spark', 'is', 'fast', 'It', 'supports', 'many', 'operations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter\n",
        "\"\"\"\n",
        "Returns elements that satisfy the predicate function.\n",
        "Filtering out null or empty values in datasets\n",
        "Selecting records that meet certain criteria (e.g., age > 30)\n",
        "Removing bad or incomplete records\n",
        "\"\"\"\n",
        "\n",
        "words = spark.sparkContext.parallelize([\"cat\", \"elephant\", \"rat\", \"dog\", \"giraffe\"])\n",
        "\n",
        "# Filter words with length greater than 3\n",
        "long_words = words.filter(lambda word: len(word) > 3)\n",
        "\n",
        "# Collect and print the result\n",
        "print(long_words.collect())\n",
        "\n",
        "\n",
        "filt = words.filter(lambda x: x != \"cat\")\n",
        "\n",
        "print(filt.collect())\n",
        "\n",
        "fil_cat = words.filter(lambda x: x == \"cat\")\n",
        "\n",
        "print(fil_cat.collect())\n",
        "\n",
        "diff_char = words.filter(lambda x: x != \"cat\" and x != \"dog\")\n",
        "\n",
        "print(diff_char.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpUBWyiFdWj4",
        "outputId": "06f2b1ff-0dd8-4266-b8fc-7822c33b7c90"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['elephant', 'giraffe']\n",
            "['elephant', 'rat', 'dog', 'giraffe']\n",
            "['cat']\n",
            "['elephant', 'rat', 'giraffe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "#distinct\n",
        "\"\"\"\n",
        "Removes duplicate elements.\n",
        "\n",
        "The distinct() transformation removes duplicate elements from an RDD. It returns a new RDD that contains only the unique elements.\n",
        "distinct() involves a shuffle operation, which may impact performance for large datasets. If performance is critical, consider combining it with map()\n",
        "and reduceByKey() for custom deduplication.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "mylist = [1,1,2,2,3,3,4,5,6,7]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(mylist)\n",
        "\n",
        "print(rdd.collect())\n",
        "\n",
        "print(rdd.distinct().collect())\n",
        "\n",
        "print(rdd.sortBy(lambda x: x).collect())\n",
        "\n",
        "print(rdd.distinct().sortBy(lambda x: x, ascending=True).collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9wSpEzRe4Ec",
        "outputId": "01515aa9-4fc8-4a5b-828b-6108be933d00"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 2, 3, 3, 4, 5, 6, 7]\n",
            "[2, 4, 6, 1, 3, 5, 7]\n",
            "[1, 1, 2, 2, 3, 3, 4, 5, 6, 7]\n",
            "[1, 2, 3, 4, 5, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#union\n",
        "\"\"\"\n",
        "The union() transformation in Apache Spark combines two RDDs into a single RDD that contains all elements from both.\n",
        "\n",
        "It does not remove duplicates ‚Äî use distinct() afterward if needed.\n",
        "Both RDDs should have the same data type.\n",
        "\n",
        "Returns the union of two RDDs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1, 2, 3])\n",
        "rdd2 = spark.sparkContext.parallelize([3, 4, 5])\n",
        "\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "\n",
        "print(union_rdd.collect())\n",
        "\n",
        "print(\"after distinct\")\n",
        "distinct_combined = union_rdd.distinct()\n",
        "print(distinct_combined.sortBy(lambda x: x, ascending=True).collect())\n",
        "# Output: [1, 2, 3, 4, 5]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEubx1G-gzYN",
        "outputId": "133b2d82-f709-4d03-bd2f-f8a1c74625ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 3, 4, 5]\n",
            "after distinct\n",
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#intersection\n",
        "\"\"\"\n",
        "The intersection() transformation in Spark is used to return a new RDD that contains the common elements between two RDDs (i.e., the set intersection).\n",
        "\n",
        "rdd1.intersection(rdd2): Compares both RDDs and keeps only the elements that appear in both.\n",
        "It removes duplicates automatically, just like a mathematical set intersection.\n",
        "\n",
        "intersection() can trigger a shuffle operation, which may be costly for large datasets.\n",
        "Use it only when necessary, especially in distributed environments.\n",
        "\"\"\"\n",
        "rdd1 = spark.sparkContext.parallelize([1, 2, 3,4])\n",
        "rdd2 = spark.sparkContext.parallelize([3, 4, 5,6])\n",
        "\n",
        "intersection_rdd = rdd1.intersection(rdd2)\n",
        "\n",
        "print(intersection_rdd.collect())  # Output: [3, 4]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJj9Gfh-ixTQ",
        "outputId": "ee674bd3-ab9e-450b-cea5-cca784c5354d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#subtract\n",
        "\"\"\"\n",
        "In Apache Spark RDDs, subtract() is a transformation used to remove elements present in another RDD. It performs a set difference operation.\n",
        "\n",
        "Definition: Returns an RDD with elements from the first RDD that are not in the second RDD.\n",
        "Use case: Filtering out unwanted data or comparing datasets.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1, 2, 3,4,5])\n",
        "rdd2 = spark.sparkContext.parallelize([3, 4, 5,6,7])\n",
        "\n",
        "subt_rdd = rdd1.subtract(rdd2)\n",
        "\n",
        "print(subt_rdd.collect())\n",
        "\n",
        "\"\"\"\n",
        "rdd1: [1, 2, 3, 4, 5]\n",
        "rdd2: [3, 4, 5, 6, 7]\n",
        "rdd1.subtract(rdd2): Returns elements in rdd1 that are not in rdd2 ‚Üí [1, 2]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "0P3jKiq8jk6f",
        "outputId": "5b76c03d-69fc-4956-aad5-5f6114c7c0e4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nrdd1: [1, 2, 3, 4, 5]\\nrdd2: [3, 4, 5, 6, 7]\\nrdd1.subtract(rdd2): Returns elements in rdd1 that are not in rdd2 ‚Üí [1, 2]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cartesian\n",
        "\"\"\"\n",
        "The cartesian() transformation in Apache Spark returns the Cartesian product of two RDDs ‚Äî meaning it returns all possible pairs of elements, one from each RDD.\n",
        "\n",
        "rdd1.cartesian(rdd2) pairs each element in rdd1 with every element in rdd2.\n",
        "Result is a new RDD with tuple pairs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1,2])\n",
        "rdd2 = spark.sparkContext.parallelize(['a','b','c'])\n",
        "\n",
        "cartesian_rdd = rdd1.cartesian(rdd2)\n",
        "\n",
        "print(cartesian_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT1fFf5jkf-A",
        "outputId": "d13fe483-819b-498f-b316-8252d15825a8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#groupby : Groups values with the same key.\n",
        "\n",
        "\"\"\"\n",
        " groupBy() (or more commonly, groupByKey() for key-value RDDs) is a transformation that groups elements sharing the same key.\n",
        " It does not perform aggregation ‚Äî it simply groups values, and then you can perform operations like sum, count, etc., afterward.\n",
        "\n",
        " Used on (key, value) pair RDDs.\n",
        " Groups all values with the same key into a single list.\n",
        " Often followed by a function like mapValues() or reduce() to perform aggregation (e.g., add).\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (\"apple\", 3),\n",
        "    (\"banana\", 2),\n",
        "    (\"apple\", 4),\n",
        "    (\"banana\", 1),\n",
        "    (\"orange\", 5) ])\n",
        "\n",
        "group_rdd = rdd.groupByKey()\n",
        "\n",
        "print(group_rdd.collect())\n",
        "\n",
        "print(\"\\n group key values\")\n",
        "for key, values in group_rdd.collect():\n",
        " print(f\"Key: {key}, Values: {list(values)}\")\n",
        "\n",
        "print(\"\\n sum of values\")\n",
        "data = group_rdd.mapValues(lambda x: sum(x))\n",
        "\n",
        "print(data.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LipjE-g3uZQ4",
        "outputId": "9e176b88-0e5d-4863-fda4-801a3d49bfdb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apple', <pyspark.resultiterable.ResultIterable object at 0x7d5967461b10>), ('banana', <pyspark.resultiterable.ResultIterable object at 0x7d595326ca50>), ('orange', <pyspark.resultiterable.ResultIterable object at 0x7d5953359750>)]\n",
            "\n",
            " group key values\n",
            "Key: apple, Values: [3, 4]\n",
            "Key: banana, Values: [2, 1]\n",
            "Key: orange, Values: [5]\n",
            "\n",
            " sum of values\n",
            "[('apple', 7), ('banana', 3), ('orange', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reduceByKey\n",
        "\n",
        "\"\"\"\n",
        "Merges values with the same key using the specified function.\n",
        "\n",
        "reduceByKey is a transformation used on key-value (pair) RDDs in Apache Spark. It merges the values for each key using a specified reduce function.\n",
        "\n",
        "This is useful when you want to aggregate data by key, like summing numbers for each word or category.\n",
        "\n",
        "rdd.reduceByKey(func)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (\"apple\", 1),\n",
        "    (\"banana\", 2),\n",
        "    (\"apple\", 4),\n",
        "    (\"banana\", 1),\n",
        "    (\"orange\", 5) ])\n",
        "\n",
        "\n",
        "red_rdd = rdd.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "print(red_rdd.collect())\n",
        "\n",
        "#word count\n",
        "\n",
        "a = \"hello this is a word count hello this is a word count\"\n",
        "print('\\n', a)\n",
        "\n",
        "word = spark.sparkContext.parallelize(a.split())\n",
        "#split\n",
        "print(\"\\n\", word.collect())\n",
        "#key& value\n",
        "word_count = word.map(lambda x: (x,1))\n",
        "\n",
        "print('\\n' , word_count.collect())\n",
        "\n",
        "#group key and values\n",
        "\n",
        "grp_rdd = word_count.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "print(\"\\n\", grp_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbG-X_iyLly",
        "outputId": "25094785-a8b3-4fe4-99c1-60104d97531f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apple', 5), ('banana', 3), ('orange', 5)]\n",
            "\n",
            " hello this is a word count hello this is a word count\n",
            "\n",
            " ['hello', 'this', 'is', 'a', 'word', 'count', 'hello', 'this', 'is', 'a', 'word', 'count']\n",
            "\n",
            " [('hello', 1), ('this', 1), ('is', 1), ('a', 1), ('word', 1), ('count', 1), ('hello', 1), ('this', 1), ('is', 1), ('a', 1), ('word', 1), ('count', 1)]\n",
            "\n",
            " [('hello', 2), ('this', 2), ('word', 2), ('is', 2), ('a', 2), ('count', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sorts RDD by key.\n",
        "\n",
        "\"\"\"\n",
        "Sorts RDD by key.\n",
        "\n",
        "sortBy() is a transformation in Apache Spark used to sort the elements of an RDD based on a given key or function.\n",
        "\n",
        "rdd.sortBy(keyfunc, ascending=True, numPartitions=None)\n",
        "\n",
        "keyfunc: A function to extract the key for sorting.\n",
        "ascending: Sort order (default is True).\n",
        "numPartitions: Number of partitions after sort (optional).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1,2,3,8,9,10,12,13])\n",
        "rdd2 = spark.sparkContext.parallelize([\"apple\",\"abs\",\"acd\"])\n",
        "\n",
        "sort_rdd = rdd1.sortBy(lambda x: x)\n",
        "\n",
        "print(sort_rdd.collect())\n",
        "\n",
        "sort_rdd2 = rdd2.sortBy(lambda x: x)\n",
        "\n",
        "print(sort_rdd2.collect())\n",
        "\n",
        "#sort by squre\n",
        "sort_rdd3 = rdd1.sortBy(lambda x: x ** 2)\n",
        "\n",
        "print(sort_rdd3.collect())\n",
        "\n",
        "sort_rdd4 = rdd1.sortBy(lambda x: x, ascending=False, numPartitions=2)\n",
        "\n",
        "print(sort_rdd4.collect())\n",
        "\n",
        "print(sort_rdd4.getNumPartitions())\n",
        "\n",
        "sort_rdd5 = rdd1.sortBy(lambda x: x, ascending=True, numPartitions=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlYzjwhM0pya",
        "outputId": "7062c5aa-5043-482b-fbcb-08858f268436"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 8, 9, 10, 12, 13]\n",
            "['abs', 'acd', 'apple']\n",
            "[1, 2, 3, 8, 9, 10, 12, 13]\n",
            "[13, 12, 10, 9, 8, 3, 2, 1]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join:Joins two RDDs by key.\n",
        "\n",
        "\"\"\"\n",
        "In Apache Spark, the join() transformation is used to combine two RDDs based on keys, similar to a SQL INNER JOIN.\n",
        "It's used with key-value pair RDDs ((key, value) format).\n",
        "join() is used to combine two key-value pair RDDs (i.e., RDDs of the form (key, value)) based on matching keys.\n",
        "\n",
        "It performs an inner join by default ‚Äî only keys that are present in both RDDs will appear in the result.\n",
        "\n",
        "joined_rdd = rdd1.join(rdd2)\n",
        "Where:\n",
        "\n",
        "rdd1: RDD with pairs like (K, V1)\n",
        "rdd2: RDD with pairs like (K, V2)\n",
        "Result: RDD of (K, (V1, V2)) for keys common in both RDDs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"a\", \"apple\"), (\"b\", \"banana\"), (\"d\", \"dragonfruit\")])\n",
        "\n",
        "joined_rdd = rdd1.join(rdd2)\n",
        "\n",
        "print(joined_rdd.collect())\n",
        "\n",
        "ljoin_rdd = rdd1.leftOuterJoin(rdd2)\n",
        "\n",
        "print(ljoin_rdd.collect())\n",
        "\n",
        "rjoin_rdd = rdd1.rightOuterJoin(rdd2)\n",
        "\n",
        "print(rjoin_rdd.collect())\n",
        "\n",
        "full_jion_rdd = rdd1.fullOuterJoin(rdd2)\n",
        "\n",
        "print(full_jion_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgEW_AmE3hJK",
        "outputId": "3a16b3e6-9a8c-4b20-ffe5-915936263eab"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', (2, 'banana')), ('a', (1, 'apple'))]\n",
            "[('b', (2, 'banana')), ('c', (3, None)), ('a', (1, 'apple'))]\n",
            "[('d', (None, 'dragonfruit')), ('b', (2, 'banana')), ('a', (1, 'apple'))]\n",
            "[('d', (None, 'dragonfruit')), ('b', (2, 'banana')), ('c', (3, None)), ('a', (1, 'apple'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#coalesce(numPartitions)\tReduces the number of partitions.\n",
        "\n",
        "\"\"\"\n",
        "The coalesce() transformation reduces the number of partitions in an RDD. It is commonly used for optimizing performance before saving to disk\n",
        "(especially after wide transformations or when writing small output files).\n",
        "\n",
        "RDD.coalesce(numPartitions, shuffle=False)\n",
        "numPartitions: The number of partitions you want.\n",
        "shuffle (optional): If True, allows reshuffling of data for better distribution. By default, it is False (no shuffle, just merges adjacent partitions).\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data = spark.sparkContext.parallelize(range(1,20),6) #creates an RDD with 6 partitions\n",
        "\n",
        "print(data.getNumPartitions())\n",
        "print(data.glom().collect())\n",
        "\n",
        "coalesce_rdd = data.coalesce(2) #reduce to 2 partions\n",
        "\n",
        "print(coalesce_rdd.getNumPartitions())\n",
        "print(coalesce_rdd.glom().collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woKFZaiK4-ja",
        "outputId": "b2065268-6244-4781-e758-af242066f478"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18, 19]]\n",
            "2\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#repartition(numPartitions)\tReshuffles data into a specified number of partitions.\n",
        "\n",
        "\"\"\"\n",
        "repartition(numPartitions) is a transformation in Spark that reshuffles the data to increase or decrease the number of partitions in an RDD.\n",
        "\n",
        "It performs a full shuffle, which means it can be expensive in terms of performance, but useful for load balancing or preparing data for\n",
        "further operations like joins or saves.\n",
        "\n",
        "rdd.repartition(numPartitions)\n",
        "\n",
        "When to Use repartition():\n",
        "\n",
        "When you want more parallelism by increasing partitions\n",
        "After a coalesce() call that reduced partitions too much\n",
        "Before a write to evenly distribute data across output files\n",
        "\"\"\"\n",
        "\n",
        "data = spark.sparkContext.parallelize(range(1,20), numSlices=2)# 2 partitions\n",
        "\n",
        "print(data.getNumPartitions())\n",
        "print(data.glom().collect())\n",
        "\n",
        "repart_rdd = data.repartition(4) #reshuffle to slice of 4\n",
        "\n",
        "print('\\n', repart_rdd.getNumPartitions())\n",
        "print(repart_rdd.glom().collect())\n",
        "\n",
        "\"\"\"\n",
        "‚ö†Ô∏è Note\n",
        "If you're reducing partitions (e.g. from 10 to 2), prefer coalesce() instead of repartition() ‚Äî it's more efficient because it avoids full data shuffle.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print('\\n',repart_rdd.coalesce(2).glom().collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66LM7syj6hDZ",
        "outputId": "d568975c-3344-4bf6-e8ab-c9ad143ae443"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
            "\n",
            " 4\n",
            "[[], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [], []]\n",
            "\n",
            " [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], []]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import pipe\n",
        "#pipe(command)\tPipes each partition through an external command.\n",
        "\n",
        "\"\"\"\n",
        "The pipe() transformation in Spark RDDs is used to run shell commands or external scripts on each RDD partition.\n",
        "It allows piping partition data through an external process, often used when integrating with command-line tools or legacy systems.\n",
        "\n",
        "üìò Syntax\n",
        "\n",
        "rdd.pipe(command)\n",
        "\n",
        "command: A string representing the shell command to run.\n",
        "Each partition‚Äôs data is sent to the external command via standard input, and the command‚Äôs standard output becomes the new RDD.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data = spark.sparkContext.parallelize([\n",
        "    \"hello this is a lines \",\n",
        "    \"this is new\",\n",
        "    \"this is aswome\" ], numSlices=2)\n",
        "\n",
        "pip_rdd = data.pipe(\"grep this\")\n",
        "\n",
        "\n",
        "print(pip_rdd.collect())\n",
        "\n",
        "\"\"\"\n",
        "‚ö†Ô∏è Notes\n",
        "\n",
        "The command runs independently on each partition.\n",
        "The external tool must be installed and accessible on every executor node.\n",
        "Data is streamed via stdin/stdout, so it's line-oriented.\n",
        "It‚Äôs not portable across platforms (Windows/Linux) or clusters without consistent environment setup.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "nJv94lkF8jL0",
        "outputId": "5ff9352f-9df0-4fbb-8f09-ced010438ff4"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello this is a lines ', 'this is new', 'this is aswome']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n‚ö†Ô∏è Notes\\n\\nThe command runs independently on each partition.\\nThe external tool must be installed and accessible on every executor node.\\nData is streamed via stdin/stdout, so it's line-oriented.\\nIt‚Äôs not portable across platforms (Windows/Linux) or clusters without consistent environment setup.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mapPartitions(func)\tApplies a function to each partition.\n",
        "\"\"\"\n",
        "\n",
        "The mapPartitions() transformation applies a function to each partition of the RDD (not to each element like map()), which can be more efficient when working with large datasets or external connections like databases.\n",
        "\n",
        "‚úÖ Syntax\n",
        "RDD.mapPartitions(f)\n",
        "Why Use mapPartitions()?\n",
        "Performance: Reduces function call overhead by applying once per partition.\n",
        "Resource Sharing: Ideal when you need to open a DB connection or expensive resource once per partition, not for every record.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "data = spark.sparkContext.parallelize([1,2,3,4,5], numSlices=2)\n",
        "print(data)\n",
        "\n",
        "\n",
        "mp_rdd = data.mapPartitions(lambda x: [i*2 for i in x])\n",
        "\n",
        "print(mp_rdd.collect())\n",
        "\n",
        "print(\"\\n using function:\")\n",
        "def double(partition):#function\n",
        "  return [i*2 for i in partition]\n",
        "\n",
        "mp_rdd2 = data.mapPartitions(double)#functon call\n",
        "\n",
        "print('\\n', mp_rdd2.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4891UoD-E8A",
        "outputId": "9e9039cd-5467-4e3c-86e4-1a02285724b8"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[782] at readRDDFromFile at PythonRDD.scala:289\n",
            "[2, 4, 6, 8, 10]\n",
            "\n",
            " using function:\n",
            "\n",
            " [2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample(withReplacement, fraction)\tSamples a fraction of the data.\n",
        "\n",
        "\"\"\"\n",
        "üìò sample() in Spark RDDs\n",
        "The sample() transformation in Apache Spark is used to extract a random sample from an RDD.\n",
        "\n",
        "üß™ Syntax\n",
        "RDD.sample(withReplacement, fraction, seed=None)\n",
        "withReplacement (bool) ‚Äì Can elements be selected more than once? (True = yes)\n",
        "fraction (float) ‚Äì Approximate fraction of the dataset to sample (e.g., 0.1 = 10%)\n",
        "seed (int, optional) ‚Äì Random seed for reproducibility\n",
        "\n",
        "If withReplacement=True, some elements may appear multiple times.\n",
        "If withReplacement=False, it's more like a subset of the original data.\n",
        "Useful for testing, prototyping, or stratified sampling from large datasets.\n",
        "\n",
        "TypeError: RDD.sample() missing 2 required positional arguments: 'withReplacement' and 'fraction'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "data = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "sample_rdd = data.sample(withReplacement=False, fraction=0.5)\n",
        "\n",
        "print(sample_rdd.collect())\n",
        "\n",
        "sample_rdd1 = data.sample(withReplacement=True, fraction=0.6)\n",
        "\n",
        "print(sample_rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10mPelWYDyiF",
        "outputId": "055fcf6c-5b29-4bd2-e64a-6b35f9b75946"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 4, 5]\n",
            "[4, 4]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx/IcpznpVNtuo0XCa2Ql8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}