{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kptej/MyLearning/blob/main/spark_rdd_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rffP1dgOrdJt",
        "outputId": "54328ebd-5ac6-4888-c152-08545c38bf53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O_0IeQctjl9"
      },
      "source": [
        "**spark RDD**\n",
        "\n",
        "RDD (Resilient Distributed Dataset) is a core building block of PySpark. It is a fault-tolerant, immutable, distributed collection of objects. Immutable means that once you create an RDD, you cannot change it. The data within RDDs is segmented into logical partitions, allowing for distributed computation across multiple nodes within the cluster.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU6kY3Jkr25Z",
        "outputId": "8f222d37-2657-4031-ffa9-831d19f0f273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7fa62271ab90>\n",
            "Spark_RDD_examples\n",
            "<pyspark.conf.SparkConf object at 0x7fa63aede690>\n",
            "<SparkContext master=local[*] appName=Spark_RDD_examples>\n",
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n",
            "[1, 2, 3, 4, 5]\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "#create sparkcontext using spark, and read the rdd list\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Spark_RDD_examples\").getOrCreate()\n",
        "\n",
        "print(spark)\n",
        "print(spark.sparkContext.appName)\n",
        "print(spark.sparkContext.getConf())\n",
        "print(spark.sparkContext)\n",
        "\n",
        "#create rdd\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "#print RDD\n",
        "print(rdd)\n",
        "#collect RDD\n",
        "print(rdd.collect())\n",
        "print(rdd.count())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgz59JfQ2Hpj",
        "outputId": "e92d883e-31aa-439d-d01c-2d811b9f11b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "print(rdd.collect())\n",
        "\n",
        "print(rdd.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CZhSBFVZk6t",
        "outputId": "0549f319-a7d2-4acb-d380-700dcae17d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method SparkContext.emptyRDD of <SparkContext master=local[*] appName=Spark_RDD_examples>>\n"
          ]
        }
      ],
      "source": [
        "#empty Rdd\n",
        "rdd = spark.sparkContext.emptyRDD\n",
        "\n",
        "print(rdd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#rdd Tranformations\n",
        "\n",
        "\n",
        "Apache Spark's Resilient Distributed Datasets (RDDs) support two primary types of operations: Transformations and Actions. Transformations are lazy operations that define a new RDD from an existing one, while Actions trigger the execution of these transformations and return results\n",
        "\n",
        "**RDD Transformations**\n",
        "\n",
        "Transformations are operations that create a new RDD from an existing one. They are evaluated lazily, meaning computation is deferred until an action requires the result. Transformations can be categorized into:\n",
        "\n",
        "**Narrow Transformations:** Each output partition depends on a single input partition (e.g., map, filter).\n",
        "\n",
        "**Wide Transformations:** Output partitions depend on multiple input partitions, often requiring data shuffling across the cluster (e.g., reduceByKey, join)\n",
        "\n",
        "| Transformation                      | Description                                                               |                                                                                                                                  |\n",
        "| ----------------------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `map(func)`                         | Applies a function to each element, returning a new RDD.                  |                                                                                                                                  |\n",
        "| `flatMap(func)`                     | Similar to `map`, but can return multiple output elements for each input. |                                                                                                                                  |\n",
        "| `filter(func)`                      | Returns elements that satisfy the predicate function.                     |                                                                                                                                  |\n",
        "| `distinct()`                        | Removes duplicate elements.                                               |                                                                                                                                  |\n",
        "| `union(otherRDD)`                   | Returns the union of two RDDs.                                            |                                                                                                                                  |\n",
        "| `intersection(otherRDD)`            | Returns the intersection of two RDDs.                                     |                                                                                                                                  |\n",
        "| `subtract(otherRDD)`                | Returns elements present in the first RDD but not in the second.          |                                                                                                                                  |\n",
        "| `cartesian(otherRDD)`               | Returns the Cartesian product of two RDDs.                                |                                                                                                                                  |\n",
        "| `groupByKey()`                      | Groups values with the same key.                                          |                                                                                                                                  |\n",
        "| `reduceByKey(func)`                 | Merges values with the same key using the specified function.             |                                                                                                                                  |\n",
        "| `sortByKey()`                       | Sorts RDD by key.                                                         |                                                                                                                                  |\n",
        "| `join(otherRDD)`                    | Joins two RDDs by key.                                                    |                                                                                                                                  |\n",
        "| `coalesce(numPartitions)`           | Reduces the number of partitions.                                         |                                                                                                                                  |\n",
        "| `repartition(numPartitions)`        | Reshuffles data into a specified number of partitions.                    |                                                                                                                                  |\n",
        "| `pipe(command)`                     | Pipes each partition through an external command.                         |                                                                                                                                  |\n",
        "| `mapPartitions(func)`               | Applies a function to each partition.                                     |                                                                                                                                  |\n",
        "| `sample(withReplacement, fraction)` | Samples a fraction of the data.                                           | ([sparkcodehub.com][1], [LinkedIn][2], [Apache Spark][3], [Stack Overflow][4], [DataFlair][5], [Medium][6], [Stack Overflow][7]) |\n",
        "\n",
        "[1]: https://www.sparkcodehub.com/spark/rdd/transformations?utm_source=chatgpt.com \"Mastering Apache Spark RDD Transformations - SparkCodeHub\"\n",
        "[2]: https://www.linkedin.com/pulse/spark-transformations-actions-lazy-evaluation-mohammad-younus-jameel?utm_source=chatgpt.com \"Spark Transformations, Actions and Lazy Evaluation. - LinkedIn\"\n",
        "[3]: https://spark.apache.org/docs/latest/rdd-programming-guide.html?utm_source=chatgpt.com \"RDD Programming Guide - Spark 3.5.5 Documentation\"\n",
        "[4]: https://stackoverflow.com/questions/45908291/rdd-transformation-and-actions?utm_source=chatgpt.com \"RDD transformation and actions - apache spark - Stack Overflow\"\n",
        "[5]: https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/?utm_source=chatgpt.com \"Spark RDD Operations-Transformation & Action with Example\"\n",
        "[6]: https://medium.com/%40sujathamudadla1213/spark-transformations-and-actions-ff4b576cbef8?utm_source=chatgpt.com \"Spark RDD Transformations and Actions. | by Sujatha Mudadla\"\n",
        "[7]: https://stackoverflow.com/questions/78722890/where-can-i-find-an-exhaustive-list-of-actions-for-spark?utm_source=chatgpt.com \"Where can I find an exhaustive list of actions for spark?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "cv6XbyWmZUxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rdd Tranformations\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Spark_RDD_examples\").getOrCreate()\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "\n",
        "print(rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAfHk07QZIyq",
        "outputId": "8f99421e-21bb-4c5c-bd05-48012975a10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#map:Applies a function to each element, returning a new RDD.\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create an RDD from a Python list\n",
        "numbers = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Use the map transformation to square each element\n",
        "squared_numbers = numbers.map(lambda x: x ** 2)\n",
        "\n",
        "num = numbers.map(lambda x: (x,1) )\n",
        "\n",
        "print(num.collect())\n",
        "# Collect the results and print\n",
        "print(squared_numbers.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX0QzP7xZqsT",
        "outputId": "98efd026-0d3f-45b6-cd5f-705a5587d635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
            "[1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flat map\n",
        "# flatMap() in Spark with an RDD ‚Äî it's often used to split elements into multiple parts, like splitting sentences into words.\n",
        "#üîç Key Differences from map()\n",
        "\"\"\" map() would return a list of lists: one list per sentence.\n",
        "flatMap() flattens those lists into a single list of words.\"\"\"\n",
        "\n",
        "# Create an RDD of sentences\n",
        "sentences = spark.sparkContext.parallelize([\n",
        "    \"Apache Spark is fast\",\n",
        "    \"It supports many operations\"\n",
        "])\n",
        "\n",
        "# Use flatMap to split each sentence into words\n",
        "words = sentences.flatMap(lambda sentence: sentence.split())\n",
        "\n",
        "w = sentences.flatMap(lambda x: x.split())\n",
        "\n",
        "print(w.foreach(lambda x : print(x)))\n",
        "\n",
        "# Collect and print the words\n",
        "print(words.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY1n19LFafNP",
        "outputId": "d399bf95-e1fb-4229-8b9d-80bb964fc49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "['Apache', 'Spark', 'is', 'fast', 'It', 'supports', 'many', 'operations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter\n",
        "\"\"\"\n",
        "Returns elements that satisfy the predicate function.\n",
        "Filtering out null or empty values in datasets\n",
        "Selecting records that meet certain criteria (e.g., age > 30)\n",
        "Removing bad or incomplete records\n",
        "\"\"\"\n",
        "\n",
        "words = spark.sparkContext.parallelize([\"cat\", \"elephant\", \"rat\", \"dog\", \"giraffe\"])\n",
        "\n",
        "# Filter words with length greater than 3\n",
        "long_words = words.filter(lambda word: len(word) > 3)\n",
        "\n",
        "# Collect and print the result\n",
        "print(long_words.collect())\n",
        "\n",
        "\n",
        "filt = words.filter(lambda x: x != \"cat\")\n",
        "\n",
        "print(filt.collect())\n",
        "\n",
        "fil_cat = words.filter(lambda x: x == \"cat\")\n",
        "\n",
        "print(fil_cat.collect())\n",
        "\n",
        "diff_char = words.filter(lambda x: x != \"cat\" and x != \"dog\")\n",
        "\n",
        "print(diff_char.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpUBWyiFdWj4",
        "outputId": "06f2b1ff-0dd8-4266-b8fc-7822c33b7c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['elephant', 'giraffe']\n",
            "['elephant', 'rat', 'dog', 'giraffe']\n",
            "['cat']\n",
            "['elephant', 'rat', 'giraffe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "#distinct\n",
        "\"\"\"\n",
        "Removes duplicate elements.\n",
        "\n",
        "The distinct() transformation removes duplicate elements from an RDD. It returns a new RDD that contains only the unique elements.\n",
        "distinct() involves a shuffle operation, which may impact performance for large datasets. If performance is critical, consider combining it with map()\n",
        "and reduceByKey() for custom deduplication.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "mylist = [1,1,2,2,3,3,4,5,6,7]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(mylist)\n",
        "\n",
        "print(rdd.collect())\n",
        "\n",
        "print(rdd.distinct().collect())\n",
        "\n",
        "print(rdd.sortBy(lambda x: x).collect())\n",
        "\n",
        "print(rdd.distinct().sortBy(lambda x: x, ascending=True).collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9wSpEzRe4Ec",
        "outputId": "01515aa9-4fc8-4a5b-828b-6108be933d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 2, 3, 3, 4, 5, 6, 7]\n",
            "[2, 4, 6, 1, 3, 5, 7]\n",
            "[1, 1, 2, 2, 3, 3, 4, 5, 6, 7]\n",
            "[1, 2, 3, 4, 5, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#union\n",
        "\"\"\"\n",
        "The union() transformation in Apache Spark combines two RDDs into a single RDD that contains all elements from both.\n",
        "\n",
        "It does not remove duplicates ‚Äî use distinct() afterward if needed.\n",
        "Both RDDs should have the same data type.\n",
        "\n",
        "Returns the union of two RDDs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1, 2, 3])\n",
        "rdd2 = spark.sparkContext.parallelize([3, 4, 5])\n",
        "\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "\n",
        "print(union_rdd.collect())\n",
        "\n",
        "print(\"after distinct\")\n",
        "distinct_combined = union_rdd.distinct()\n",
        "print(distinct_combined.sortBy(lambda x: x, ascending=True).collect())\n",
        "# Output: [1, 2, 3, 4, 5]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEubx1G-gzYN",
        "outputId": "133b2d82-f709-4d03-bd2f-f8a1c74625ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 3, 4, 5]\n",
            "after distinct\n",
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#intersection\n",
        "\"\"\"\n",
        "The intersection() transformation in Spark is used to return a new RDD that contains the common elements between two RDDs (i.e., the set intersection).\n",
        "\n",
        "rdd1.intersection(rdd2): Compares both RDDs and keeps only the elements that appear in both.\n",
        "It removes duplicates automatically, just like a mathematical set intersection.\n",
        "\n",
        "intersection() can trigger a shuffle operation, which may be costly for large datasets.\n",
        "Use it only when necessary, especially in distributed environments.\n",
        "\"\"\"\n",
        "rdd1 = spark.sparkContext.parallelize([1, 2, 3,4])\n",
        "rdd2 = spark.sparkContext.parallelize([3, 4, 5,6])\n",
        "\n",
        "intersection_rdd = rdd1.intersection(rdd2)\n",
        "\n",
        "print(intersection_rdd.collect())  # Output: [3, 4]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJj9Gfh-ixTQ",
        "outputId": "ee674bd3-ab9e-450b-cea5-cca784c5354d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#subtract\n",
        "\"\"\"\n",
        "In Apache Spark RDDs, subtract() is a transformation used to remove elements present in another RDD. It performs a set difference operation.\n",
        "\n",
        "Definition: Returns an RDD with elements from the first RDD that are not in the second RDD.\n",
        "Use case: Filtering out unwanted data or comparing datasets.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1, 2, 3,4,5])\n",
        "rdd2 = spark.sparkContext.parallelize([3, 4, 5,6,7])\n",
        "\n",
        "subt_rdd = rdd1.subtract(rdd2)\n",
        "\n",
        "print(subt_rdd.collect())\n",
        "\n",
        "\"\"\"\n",
        "rdd1: [1, 2, 3, 4, 5]\n",
        "rdd2: [3, 4, 5, 6, 7]\n",
        "rdd1.subtract(rdd2): Returns elements in rdd1 that are not in rdd2 ‚Üí [1, 2]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "0P3jKiq8jk6f",
        "outputId": "5b76c03d-69fc-4956-aad5-5f6114c7c0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nrdd1: [1, 2, 3, 4, 5]\\nrdd2: [3, 4, 5, 6, 7]\\nrdd1.subtract(rdd2): Returns elements in rdd1 that are not in rdd2 ‚Üí [1, 2]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cartesian\n",
        "\"\"\"\n",
        "The cartesian() transformation in Apache Spark returns the Cartesian product of two RDDs ‚Äî meaning it returns all possible pairs of elements, one from each RDD.\n",
        "\n",
        "rdd1.cartesian(rdd2) pairs each element in rdd1 with every element in rdd2.\n",
        "Result is a new RDD with tuple pairs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1,2])\n",
        "rdd2 = spark.sparkContext.parallelize(['a','b','c'])\n",
        "\n",
        "cartesian_rdd = rdd1.cartesian(rdd2)\n",
        "\n",
        "print(cartesian_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT1fFf5jkf-A",
        "outputId": "d13fe483-819b-498f-b316-8252d15825a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#groupby : Groups values with the same key.\n",
        "\n",
        "\"\"\"\n",
        " groupBy() (or more commonly, groupByKey() for key-value RDDs) is a transformation that groups elements sharing the same key.\n",
        " It does not perform aggregation ‚Äî it simply groups values, and then you can perform operations like sum, count, etc., afterward.\n",
        "\n",
        " Used on (key, value) pair RDDs.\n",
        " Groups all values with the same key into a single list.\n",
        " Often followed by a function like mapValues() or reduce() to perform aggregation (e.g., add).\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (\"apple\", 3),\n",
        "    (\"banana\", 2),\n",
        "    (\"apple\", 4),\n",
        "    (\"banana\", 1),\n",
        "    (\"orange\", 5) ])\n",
        "\n",
        "group_rdd = rdd.groupByKey()\n",
        "\n",
        "print(group_rdd.collect())\n",
        "\n",
        "print(\"\\n group key values\")\n",
        "for key, values in group_rdd.collect():\n",
        " print(f\"Key: {key}, Values: {list(values)}\")\n",
        "\n",
        "print(\"\\n sum of values\")\n",
        "data = group_rdd.mapValues(lambda x: sum(x))\n",
        "\n",
        "print(data.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LipjE-g3uZQ4",
        "outputId": "9e176b88-0e5d-4863-fda4-801a3d49bfdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apple', <pyspark.resultiterable.ResultIterable object at 0x7d5967461b10>), ('banana', <pyspark.resultiterable.ResultIterable object at 0x7d595326ca50>), ('orange', <pyspark.resultiterable.ResultIterable object at 0x7d5953359750>)]\n",
            "\n",
            " group key values\n",
            "Key: apple, Values: [3, 4]\n",
            "Key: banana, Values: [2, 1]\n",
            "Key: orange, Values: [5]\n",
            "\n",
            " sum of values\n",
            "[('apple', 7), ('banana', 3), ('orange', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reduceByKey\n",
        "\n",
        "\"\"\"\n",
        "Merges values with the same key using the specified function.\n",
        "\n",
        "reduceByKey is a transformation used on key-value (pair) RDDs in Apache Spark. It merges the values for each key using a specified reduce function.\n",
        "\n",
        "This is useful when you want to aggregate data by key, like summing numbers for each word or category.\n",
        "\n",
        "rdd.reduceByKey(func)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (\"apple\", 1),\n",
        "    (\"banana\", 2),\n",
        "    (\"apple\", 4),\n",
        "    (\"banana\", 1),\n",
        "    (\"orange\", 5) ])\n",
        "\n",
        "\n",
        "red_rdd = rdd.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "print(red_rdd.collect())\n",
        "\n",
        "#word count\n",
        "\n",
        "a = \"hello this is a word count hello this is a word count\"\n",
        "print('\\n', a)\n",
        "\n",
        "word = spark.sparkContext.parallelize(a.split())\n",
        "#split\n",
        "print(\"\\n\", word.collect())\n",
        "#key& value\n",
        "word_count = word.map(lambda x: (x,1))\n",
        "\n",
        "print('\\n' , word_count.collect())\n",
        "\n",
        "#group key and values\n",
        "\n",
        "grp_rdd = word_count.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "print(\"\\n\", grp_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbG-X_iyLly",
        "outputId": "25094785-a8b3-4fe4-99c1-60104d97531f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apple', 5), ('banana', 3), ('orange', 5)]\n",
            "\n",
            " hello this is a word count hello this is a word count\n",
            "\n",
            " ['hello', 'this', 'is', 'a', 'word', 'count', 'hello', 'this', 'is', 'a', 'word', 'count']\n",
            "\n",
            " [('hello', 1), ('this', 1), ('is', 1), ('a', 1), ('word', 1), ('count', 1), ('hello', 1), ('this', 1), ('is', 1), ('a', 1), ('word', 1), ('count', 1)]\n",
            "\n",
            " [('hello', 2), ('this', 2), ('word', 2), ('is', 2), ('a', 2), ('count', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sorts RDD by key.\n",
        "\n",
        "\"\"\"\n",
        "Sorts RDD by key.\n",
        "\n",
        "sortBy() is a transformation in Apache Spark used to sort the elements of an RDD based on a given key or function.\n",
        "\n",
        "rdd.sortBy(keyfunc, ascending=True, numPartitions=None)\n",
        "\n",
        "keyfunc: A function to extract the key for sorting.\n",
        "ascending: Sort order (default is True).\n",
        "numPartitions: Number of partitions after sort (optional).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([1,2,3,8,9,10,12,13])\n",
        "rdd2 = spark.sparkContext.parallelize([\"apple\",\"abs\",\"acd\"])\n",
        "\n",
        "sort_rdd = rdd1.sortBy(lambda x: x)\n",
        "\n",
        "print(sort_rdd.collect())\n",
        "\n",
        "sort_rdd2 = rdd2.sortBy(lambda x: x)\n",
        "\n",
        "print(sort_rdd2.collect())\n",
        "\n",
        "#sort by squre\n",
        "sort_rdd3 = rdd1.sortBy(lambda x: x ** 2)\n",
        "\n",
        "print(sort_rdd3.collect())\n",
        "\n",
        "sort_rdd4 = rdd1.sortBy(lambda x: x, ascending=False, numPartitions=2)\n",
        "\n",
        "print(sort_rdd4.collect())\n",
        "\n",
        "print(sort_rdd4.getNumPartitions())\n",
        "\n",
        "sort_rdd5 = rdd1.sortBy(lambda x: x, ascending=True, numPartitions=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlYzjwhM0pya",
        "outputId": "7062c5aa-5043-482b-fbcb-08858f268436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 8, 9, 10, 12, 13]\n",
            "['abs', 'acd', 'apple']\n",
            "[1, 2, 3, 8, 9, 10, 12, 13]\n",
            "[13, 12, 10, 9, 8, 3, 2, 1]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join:Joins two RDDs by key.\n",
        "\n",
        "\"\"\"\n",
        "In Apache Spark, the join() transformation is used to combine two RDDs based on keys, similar to a SQL INNER JOIN.\n",
        "It's used with key-value pair RDDs ((key, value) format).\n",
        "join() is used to combine two key-value pair RDDs (i.e., RDDs of the form (key, value)) based on matching keys.\n",
        "\n",
        "It performs an inner join by default ‚Äî only keys that are present in both RDDs will appear in the result.\n",
        "\n",
        "joined_rdd = rdd1.join(rdd2)\n",
        "Where:\n",
        "\n",
        "rdd1: RDD with pairs like (K, V1)\n",
        "rdd2: RDD with pairs like (K, V2)\n",
        "Result: RDD of (K, (V1, V2)) for keys common in both RDDs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"a\", \"apple\"), (\"b\", \"banana\"), (\"d\", \"dragonfruit\")])\n",
        "\n",
        "joined_rdd = rdd1.join(rdd2)\n",
        "\n",
        "print(joined_rdd.collect())\n",
        "\n",
        "ljoin_rdd = rdd1.leftOuterJoin(rdd2)\n",
        "\n",
        "print(ljoin_rdd.collect())\n",
        "\n",
        "rjoin_rdd = rdd1.rightOuterJoin(rdd2)\n",
        "\n",
        "print(rjoin_rdd.collect())\n",
        "\n",
        "full_jion_rdd = rdd1.fullOuterJoin(rdd2)\n",
        "\n",
        "print(full_jion_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgEW_AmE3hJK",
        "outputId": "3a16b3e6-9a8c-4b20-ffe5-915936263eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', (2, 'banana')), ('a', (1, 'apple'))]\n",
            "[('b', (2, 'banana')), ('c', (3, None)), ('a', (1, 'apple'))]\n",
            "[('d', (None, 'dragonfruit')), ('b', (2, 'banana')), ('a', (1, 'apple'))]\n",
            "[('d', (None, 'dragonfruit')), ('b', (2, 'banana')), ('c', (3, None)), ('a', (1, 'apple'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#coalesce(numPartitions)\tReduces the number of partitions.\n",
        "\n",
        "\"\"\"\n",
        "The coalesce() transformation reduces the number of partitions in an RDD. It is commonly used for optimizing performance before saving to disk\n",
        "(especially after wide transformations or when writing small output files).\n",
        "\n",
        "RDD.coalesce(numPartitions, shuffle=False)\n",
        "numPartitions: The number of partitions you want.\n",
        "shuffle (optional): If True, allows reshuffling of data for better distribution. By default, it is False (no shuffle, just merges adjacent partitions).\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data = spark.sparkContext.parallelize(range(1,20),6) #creates an RDD with 6 partitions\n",
        "\n",
        "print(data.getNumPartitions())\n",
        "print(data.glom().collect())\n",
        "\n",
        "coalesce_rdd = data.coalesce(2) #reduce to 2 partions\n",
        "\n",
        "print(coalesce_rdd.getNumPartitions())\n",
        "print(coalesce_rdd.glom().collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woKFZaiK4-ja",
        "outputId": "b2065268-6244-4781-e758-af242066f478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18, 19]]\n",
            "2\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#repartition(numPartitions)\tReshuffles data into a specified number of partitions.\n",
        "\n",
        "\"\"\"\n",
        "repartition(numPartitions) is a transformation in Spark that reshuffles the data to increase or decrease the number of partitions in an RDD.\n",
        "\n",
        "It performs a full shuffle, which means it can be expensive in terms of performance, but useful for load balancing or preparing data for\n",
        "further operations like joins or saves.\n",
        "\n",
        "rdd.repartition(numPartitions)\n",
        "\n",
        "When to Use repartition():\n",
        "\n",
        "When you want more parallelism by increasing partitions\n",
        "After a coalesce() call that reduced partitions too much\n",
        "Before a write to evenly distribute data across output files\n",
        "\"\"\"\n",
        "\n",
        "data = spark.sparkContext.parallelize(range(1,20), numSlices=2)# 2 partitions\n",
        "\n",
        "print(data.getNumPartitions())\n",
        "print(data.glom().collect())\n",
        "\n",
        "repart_rdd = data.repartition(4) #reshuffle to slice of 4\n",
        "\n",
        "print('\\n', repart_rdd.getNumPartitions())\n",
        "print(repart_rdd.glom().collect())\n",
        "\n",
        "\"\"\"\n",
        "‚ö†Ô∏è Note\n",
        "If you're reducing partitions (e.g. from 10 to 2), prefer coalesce() instead of repartition() ‚Äî it's more efficient because it avoids full data shuffle.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print('\\n',repart_rdd.coalesce(2).glom().collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66LM7syj6hDZ",
        "outputId": "d568975c-3344-4bf6-e8ab-c9ad143ae443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
            "\n",
            " 4\n",
            "[[], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [], []]\n",
            "\n",
            " [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], []]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import pipe\n",
        "#pipe(command)\tPipes each partition through an external command.\n",
        "\n",
        "\"\"\"\n",
        "The pipe() transformation in Spark RDDs is used to run shell commands or external scripts on each RDD partition.\n",
        "It allows piping partition data through an external process, often used when integrating with command-line tools or legacy systems.\n",
        "\n",
        "üìò Syntax\n",
        "\n",
        "rdd.pipe(command)\n",
        "\n",
        "command: A string representing the shell command to run.\n",
        "Each partition‚Äôs data is sent to the external command via standard input, and the command‚Äôs standard output becomes the new RDD.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data = spark.sparkContext.parallelize([\n",
        "    \"hello this is a lines \",\n",
        "    \"this is new\",\n",
        "    \"this is aswome\" ], numSlices=2)\n",
        "\n",
        "pip_rdd = data.pipe(\"grep this\")\n",
        "\n",
        "\n",
        "print(pip_rdd.collect())\n",
        "\n",
        "\"\"\"\n",
        "‚ö†Ô∏è Notes\n",
        "\n",
        "The command runs independently on each partition.\n",
        "The external tool must be installed and accessible on every executor node.\n",
        "Data is streamed via stdin/stdout, so it's line-oriented.\n",
        "It‚Äôs not portable across platforms (Windows/Linux) or clusters without consistent environment setup.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "nJv94lkF8jL0",
        "outputId": "5ff9352f-9df0-4fbb-8f09-ced010438ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello this is a lines ', 'this is new', 'this is aswome']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n‚ö†Ô∏è Notes\\n\\nThe command runs independently on each partition.\\nThe external tool must be installed and accessible on every executor node.\\nData is streamed via stdin/stdout, so it's line-oriented.\\nIt‚Äôs not portable across platforms (Windows/Linux) or clusters without consistent environment setup.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mapPartitions(func)\tApplies a function to each partition.\n",
        "\"\"\"\n",
        "\n",
        "The mapPartitions() transformation applies a function to each partition of the RDD (not to each element like map()), which can be more efficient when working with large datasets or external connections like databases.\n",
        "\n",
        "‚úÖ Syntax\n",
        "RDD.mapPartitions(f)\n",
        "Why Use mapPartitions()?\n",
        "Performance: Reduces function call overhead by applying once per partition.\n",
        "Resource Sharing: Ideal when you need to open a DB connection or expensive resource once per partition, not for every record.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "data = spark.sparkContext.parallelize([1,2,3,4,5], numSlices=2)\n",
        "print(data)\n",
        "\n",
        "\n",
        "mp_rdd = data.mapPartitions(lambda x: [i*2 for i in x])\n",
        "\n",
        "print(mp_rdd.collect())\n",
        "\n",
        "print(\"\\n using function:\")\n",
        "def double(partition):#function\n",
        "  return [i*2 for i in partition]\n",
        "\n",
        "mp_rdd2 = data.mapPartitions(double)#functon call\n",
        "\n",
        "print('\\n', mp_rdd2.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4891UoD-E8A",
        "outputId": "9e9039cd-5467-4e3c-86e4-1a02285724b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[782] at readRDDFromFile at PythonRDD.scala:289\n",
            "[2, 4, 6, 8, 10]\n",
            "\n",
            " using function:\n",
            "\n",
            " [2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample(withReplacement, fraction)\tSamples a fraction of the data.\n",
        "\n",
        "\"\"\"\n",
        "üìò sample() in Spark RDDs\n",
        "The sample() transformation in Apache Spark is used to extract a random sample from an RDD.\n",
        "\n",
        "üß™ Syntax\n",
        "RDD.sample(withReplacement, fraction, seed=None)\n",
        "withReplacement (bool) ‚Äì Can elements be selected more than once? (True = yes)\n",
        "fraction (float) ‚Äì Approximate fraction of the dataset to sample (e.g., 0.1 = 10%)\n",
        "seed (int, optional) ‚Äì Random seed for reproducibility\n",
        "\n",
        "If withReplacement=True, some elements may appear multiple times.\n",
        "If withReplacement=False, it's more like a subset of the original data.\n",
        "Useful for testing, prototyping, or stratified sampling from large datasets.\n",
        "\n",
        "TypeError: RDD.sample() missing 2 required positional arguments: 'withReplacement' and 'fraction'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "data = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "sample_rdd = data.sample(withReplacement=False, fraction=0.5)\n",
        "\n",
        "print(sample_rdd.collect())\n",
        "\n",
        "sample_rdd1 = data.sample(withReplacement=True, fraction=0.6)\n",
        "\n",
        "print(sample_rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10mPelWYDyiF",
        "outputId": "055fcf6c-5b29-4bd2-e64a-6b35f9b75946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 4, 5]\n",
            "[4, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cogroup\n",
        "\"\"\"\n",
        "The cogroup() transformation is used on two (or more) key-value RDDs. It groups the values of each RDD that share the same key into iterables (lists).\n",
        "üß¨ Syntax\n",
        "rdd1.cogroup(rdd2)\n",
        "Both rdd1 and rdd2 must be pair RDDs (i.e., RDDs of key-value tuples).\n",
        "The result is an RDD of the form (key, (Iterable1, Iterable2)).\n",
        "\n",
        "\"\"\"\n",
        "rdd1 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"a\", \"apple\"), (\"b\", \"banana\"), (\"c\", \"cherry\")])\n",
        "\n",
        "cogrp_rdd = rdd1.cogroup(rdd2)\n",
        "\n",
        "\n",
        "for key, (vals1, vals2) in cogrp_rdd.collect():\n",
        " print(f\"{key}: {list(vals1)}, {list(vals2)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox21qvl8eDis",
        "outputId": "4162d15e-cad2-4f65-c041-fb6c2d6bd240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b: [2], ['banana']\n",
            "c: [], ['cherry']\n",
            "a: [1, 3], ['apple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #aggregateByKey in Apache Spark (RDD)\n",
        "\"\"\"\n",
        "The aggregateByKey() transformation is used on pair RDDs (key-value pairs) to combine values with the same key using an aggregation function, similar to reduceByKey ‚Äî but with more control over:\n",
        "\n",
        "How values are combined within partitions\n",
        "How values are combined between partitions\n",
        "\n",
        "üìå Syntax\n",
        "RDD.aggregateByKey(zeroValue)(seqFunc, combFunc)\n",
        "\n",
        "zeroValue: The initial value for each key.\n",
        "seqFunc: Function to merge a value into the accumulator within a partition.\n",
        "combFunc: Function to merge accumulators from different partitions.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (\"math\", 80),\n",
        "    (\"math\", 60),\n",
        "    (\"math\", 90),\n",
        "    (\"science\", 70),\n",
        "    (\"science\", 85),\n",
        "    (\"science\", 75)], 2)\n",
        "\n",
        "\n",
        "# aggregateByKey to get (max, sum) per subject\n",
        "zero_value = (0, 0)  # (max, sum)\n",
        "\n",
        "def seq_op(acc, value):\n",
        "    return (max(acc[0], value), acc[1] + value)\n",
        "\n",
        "def comb_op(acc1, acc2):\n",
        "    return (max(acc1[0], acc2[0]), acc1[1] + acc2[1])\n",
        "\n",
        "result =  rdd.aggregateByKey(zero_value, seq_op, comb_op)     #RDD.aggregateByKey(zeroValue)(seqFunc, combFunc)\n",
        "\n",
        "print(result.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P299RmCZeJa2",
        "outputId": "56402bd0-82ef-41d6-b399-9b6f3d0de4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('math', (90, 230)), ('science', (85, 230))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#repartitionAndSortWithinPartitions\n",
        "\"\"\"\n",
        "The repartitionAndSortWithinPartitions transformation is used to repartition an\n",
        "RDD according to a custom partitioner and sort records within each partition.\n",
        "\n",
        " Syntax (PySpark)\n",
        "rdd.repartitionAndSortWithinPartitions(partitioner, ascending=True, numPartitions=None)\n",
        "partitioner: An object defining how the data is partitioned (like HashPartitioner or RangePartitioner).\n",
        "ascending: Whether to sort in ascending order (default is True).\n",
        "numPartitions: Optional. If not given, uses the default from the partitioner.\n",
        "‚ö†Ô∏è This function is available in Scala/Java natively. In PySpark, this transformation is not directly exposed, so it‚Äôs commonly used through\n",
        "sortByKey().partitionBy() as a workaround.\n",
        " \"\"\"\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.rdd import portable_hash\n",
        "\n",
        "data = [(\"cat\", 1), (\"apple\", 3), (\"banana\", 2), (\"dog\", 4)]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Sort by key and then partition manually\n",
        "sorted_rdd = rdd.sortByKey()\n",
        "partitioned_rdd = sorted_rdd.partitionBy(2)\n",
        "\n",
        "# Show results\n",
        "def show_partition(index, iterator):\n",
        "    yield f\"Partition {index}: {list(iterator)}\"\n",
        "\n",
        "print(partitioned_rdd.mapPartitionsWithIndex(show_partition).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydK2SMcCiY2T",
        "outputId": "cda32c37-fca5-4087-f314-d904c525d9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Partition 0: [('apple', 3), ('banana', 2), ('dog', 4)]\", \"Partition 1: [('cat', 1)]\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mapPartitions\n",
        "\n",
        "\"\"\"\n",
        "üîß mapPartitions() in Spark ‚Äî Syntax & Example\n",
        "‚úÖ What It Does:\n",
        "\n",
        "mapPartitions() is a transformation that applies a function to each partition of the RDD rather than to each element (as map() does). It's efficient for batch processing and when initializing expensive resources (e.g., database connections) once per partition.\n",
        "\n",
        "üìå Syntax:\n",
        "rdd.mapPartitions(func)\n",
        "func is a function that takes an iterator over a partition and returns another iterator.\n",
        "üîç Why Use mapPartitions()?\n",
        "To reuse resources like database connections, sessions, or caches.\n",
        "To optimize performance for large transformations where map() might be too fine-grained.\n",
        "To batch process data partition-wise rather than row-by-row.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5], numSlices=2)\n",
        "\n",
        "def multiply(iterator):\n",
        "  return [i*2 for i in iterator]\n",
        "\n",
        "map_rdd = rdd.mapPartitions(multiply)\n",
        "\n",
        "print(map_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhZx8I08m7QB",
        "outputId": "540eae38-b5e2-4722-9d67-e463c7ae1a47"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ RDD Actions\n",
        "\n",
        "Actions trigger the execution of transformations and return results to the driver program or write data to external storage\n",
        "\n",
        "\n",
        "| Action                                | Description                                                    |                                                                    |\n",
        "| ------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| `collect()`                           | Returns all elements of the RDD as an array to the driver.     |                                                                    |\n",
        "| `count()`                             | Returns the number of elements in the RDD.                     |                                                                    |\n",
        "| `first()`                             | Returns the first element of the RDD.                          |                                                                    |\n",
        "| `take(n)`                             | Returns the first `n` elements of the RDD.                     |                                                                    |\n",
        "| `takeSample(withReplacement, num)`    | Returns a sample of `num` elements.                            |                                                                    |\n",
        "| `reduce(func)`                        | Aggregates the elements using the specified function.          |                                                                    |\n",
        "| `fold(zeroValue)(func)`               | Aggregates elements with a zero value and a function.          |                                                                    |\n",
        "| `aggregate(zeroValue)(seqOp, combOp)` | Aggregates elements using sequence and combine functions.      |                                                                    |\n",
        "| `foreach(func)`                       | Applies a function to each element (usually for side effects). |                                                                    |\n",
        "| `saveAsTextFile(path)`                | Saves the RDD as a text file.                                  |                                                                    |\n",
        "| `saveAsSequenceFile(path)`            | Saves the RDD as a SequenceFile.                               |                                                                    |\n",
        "| `saveAsObjectFile(path)`              | Saves the RDD as a serialized Java object file.                |                                                                    |\n",
        "| `countByKey()`                        | Counts the number of elements for each key.                    |                                                                    |\n",
        "| `foreachPartition(func)`              | Applies a function to each partition.                          | ([Medium][1], [Stack Overflow][2], [Medium][3], [Apache Spark][4]) |\n",
        "\n",
        "[1]: https://medium.com/%40sujathamudadla1213/spark-transformations-and-actions-ff4b576cbef8?utm_source=chatgpt.com \"Spark RDD Transformations and Actions. | by Sujatha Mudadla\"\n",
        "[2]: https://stackoverflow.com/questions/45908291/rdd-transformation-and-actions?utm_source=chatgpt.com \"RDD transformation and actions - apache spark - Stack Overflow\"\n",
        "[3]: https://medium.com/towardsdev/transformations-and-actions-in-spark-69d63adfdd0a?utm_source=chatgpt.com \"Transformations and Actions in spark : | by shubham mishra - Medium\"\n",
        "[4]: https://spark.apache.org/docs/latest/rdd-programming-guide.html?utm_source=chatgpt.com \"RDD Programming Guide - Spark 3.5.5 Documentation\"\n"
      ],
      "metadata": {
        "id": "8j6lkcY1dV7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#collect\n",
        "\n",
        "data = [1,2,3,4,5]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "print(rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM9rbcmzj2-H",
        "outputId": "972d4920-b1a9-463b-9530-2b41a8f81bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count\n",
        "\n",
        "print(rdd.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3Iw0O4vkGxe",
        "outputId": "6259bff9-c592-4137-8758-34ca9a5d4833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first\n",
        "print(rdd.first())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htwP9UllkKnc",
        "outputId": "977d6f61-579d-4105-e54f-9540bb7f939b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#take\n",
        "\"\"\"\n",
        "\n",
        "The take(n) action returns the first n elements of the RDD as a list. It's useful when you want to quickly inspect a small portion of your dataset without collecting the entire RDD.\n",
        "\n",
        "‚úÖ Syntax\n",
        "rdd.take(n)\n",
        "n: Number of elements to return from the start of the RDD.\"\"\"\n",
        "\n",
        "print(rdd.take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss1ejV4dkRFN",
        "outputId": "78930e7b-32e1-47a1-d3e6-067815bbc37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#takesample\n",
        "\"\"\"\n",
        "üéØ takeSample() in Spark RDD\n",
        "The takeSample() action is used to randomly sample elements from an RDD.\n",
        "\n",
        "‚úÖ Syntax\n",
        "RDD.takeSample(withReplacement, num, [seed])\n",
        "withReplacement (bool): Whether to allow the same element to be chosen more than once.\n",
        "num (int): Number of elements to sample.\n",
        "seed (optional, int): Seed for random generator (for reproducibility).\n",
        "\n",
        "\"\"\"\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "print(rdd.takeSample(withReplacement=False, num=3))\n",
        "print(rdd.takeSample(withReplacement=True, num=3))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeZim8GTkoiY",
        "outputId": "2ffc1648-cc01-4abe-af35-d8a5944e2cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 4, 2]\n",
            "[5, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reduce\n",
        "\n",
        "\"\"\"\n",
        "The reduce() action in Apache Spark RDDs is used to aggregate the elements of an RDD using a binary function (a function that takes two arguments).\n",
        "\n",
        "üìò Syntax\n",
        "\n",
        "rdd.reduce(function)\n",
        "The function must be commutative and associative so that it can be computed in parallel correctly.\n",
        "It returns a single value, not an RDD.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "redu_rdd = rdd.reduce(lambda x,y: x+y)#sum of total\n",
        "\n",
        "print(redu_rdd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmrPt6QxlTdG",
        "outputId": "c7f11c88-a80a-44d0-9c49-20b7071b0fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fold\n",
        "\n",
        "\"\"\"\n",
        "The fold() action aggregates elements in an RDD using a specified function and a zero value (an initial value).\n",
        "\n",
        "üß™ Syntax\n",
        "rdd.fold(zeroValue, func)\n",
        "zeroValue: The initial value used for aggregation in each partition and also across partitions.\n",
        "func: A function that combines two values.\n",
        "üîî fold() is similar to reduce(), but with a zero value that is used in every partition.\n",
        "\n",
        "The RDD is split into partitions.\n",
        "The function (a + b) is applied in each partition starting with the zero value 0.\n",
        "The results from each partition are then folded together again using the same function and the zero value.\n",
        "\n",
        "\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"fold_example\").getOrCreate()\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "fold_rdd = rdd.fold(0, lambda x,y: x+y)\n",
        "\n",
        "print(fold_rdd)\n",
        "\n",
        "fold_rdd1 = rdd.fold(1, lambda x,y: x*y) #print(1*2*3*4*5)\n",
        "\n",
        "print(fold_rdd1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx1dPZmiQsuH",
        "outputId": "7b246c74-b571-4e22-8265-463621baa92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aggregate\n",
        "\"\"\"\n",
        "The aggregate() function in Apache Spark RDD is a powerful action used to compute results across RDD elements using different logic for local aggregation and global aggregation.\n",
        "\n",
        "üß† What is aggregate()?\n",
        "\n",
        "RDD.aggregate(zeroValue)(seqOp, combOp)\n",
        "Parameters:\n",
        "zeroValue: The initial value for the aggregation (like 0 for sum or an empty list).\n",
        "seqOp: Function to accumulate data within a partition (local aggregation).\n",
        "combOp: Function to combine results across partitions (global aggregation).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create an RDD of numbers\n",
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# zeroValue is (sum, count)\n",
        "zero_value = (0, 0)\n",
        "\n",
        "# seqOp: add current value to sum and increment count\n",
        "seq_op = lambda acc, value: (acc[0] + value, acc[1] + 1)\n",
        "print(f\"seq_op: \" , seq_op)\n",
        "\n",
        "# combOp: combine two tuples from different partitions\n",
        "comb_op = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "print(f\"comb_op :\", comb_op)\n",
        "\n",
        "# Run aggregate\n",
        "result = rdd.aggregate(zero_value, seq_op, comb_op)\n",
        "\n",
        "print(\"Sum:\", result[0])\n",
        "print(\"Count:\", result[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceUhBuWgS11t",
        "outputId": "60ebf462-699e-42d0-a1b7-f3eb7b5b22e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_op:  <function <lambda> at 0x7a0782624220>\n",
            "comb_op : <function <lambda> at 0x7a0782626fc0>\n",
            "Sum: 15\n",
            "Count: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#foreach\n",
        "\n",
        "\"\"\"\n",
        "The foreach() action applies a function to each element in the RDD, but does not return a value to the driver. It's typically used for side effects, like writing to databases, logging, or updating external systems.\n",
        "\n",
        "üß™ Syntax\n",
        "rdd.foreach(function)\n",
        "function: A function to apply to each element of the RDD.\n",
        "Unlike map(), foreach() does not return a new RDD.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "data = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "# Use foreach to print each element (will print from worker nodes, not the driver)\n",
        "data.foreach(lambda x: print(f\"Value: {x}\")) #you dont see the output\n",
        "\n",
        "for item in data.collect():\n",
        "    print(f\"Value: {item}\")\n",
        "\n",
        "\n",
        "#collect foreach data\n",
        "\n",
        "transformed_data = data.map(lambda x: x * 2)\n",
        "\n",
        "transformed_data.foreach(print)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AG4QXQGULVb",
        "outputId": "649bfb95-a389-4009-c4f7-cce47ca8cdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value: 1\n",
            "Value: 2\n",
            "Value: 3\n",
            "Value: 4\n",
            "Value: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saveAsTextFile\n",
        "\"\"\"\n",
        "The saveAsTextFile() action writes the contents of an RDD to a text file (or directory) in the distributed file system\n",
        "(like HDFS, S3, or local FS). Each element of the RDD is written as a line in the output text file.\n",
        "\n",
        "üß¨ Syntax\n",
        "rdd.saveAsTextFile(path)\n",
        "path: The output path (directory) where the files will be stored.\n",
        "Creates a directory with multiple part files (one per partition).\n",
        "üìÇ Output Structure\n",
        "In the output/fruit_names/ directory, Spark will create files like:\n",
        "\n",
        "output/\n",
        "‚îî‚îÄ‚îÄ fruit_names/\n",
        "    ‚îú‚îÄ‚îÄ part-00000\n",
        "    ‚îú‚îÄ‚îÄ part-00001\n",
        "    ‚îî‚îÄ‚îÄ _SUCCESS\n",
        "Each part-* file contains a slice of the RDD's data, one item per line.\n",
        "\"\"\"\n",
        "#dataload\n",
        "\n",
        "data = \"\"\" he saveAsTextFile() action writes the contents of an RDD to a text file (or directory) in the distributed file system\n",
        "(like HDFS, S3, or local FS). Each element of the RDD is written as a line in the output text file.\n",
        "\n",
        "üß¨ Syntax\n",
        "rdd.saveAsTextFile(path)\n",
        "path: The output path (directory) where the files will be stored.\n",
        "Creates a directory with multiple part files (one per partition).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "t2= rdd.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "t3=t2.map(lambda x: (x,1))\n",
        "\n",
        "t4=t3.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "t4.repartition(1).saveAsTextFile(\"/content/sample_data/output.txt\")\n",
        "t4.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr2OIxPDV25s",
        "outputId": "9840a76b-e3eb-4ed4-8cbb-53fef4acd752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#countbykey\n",
        "\n",
        "\"\"\"\n",
        "countByKey() is an action used with key-value RDDs (RDDs of tuples) to count the number of occurrences for each key.\n",
        "\n",
        "‚úÖ Syntax\n",
        "RDD.countByKey()\n",
        "Returns a dictionary (dict) where each key maps to its count.\n",
        "Only works on RDDs of type (K, V) (i.e., key-value pairs).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data = [(\"a\", 1), (\"b\", 1), (\"a\", 1), (\"b\", 1), (\"a\", 1)]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "count_by_key = rdd.countByKey()\n",
        "\n",
        "print(count_by_key)\n",
        "\n",
        "count_by_key.items()\n",
        "count_by_value = rdd.countByValue()\n",
        "print(count_by_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH7X9ezilhto",
        "outputId": "5071db41-ac40-4e40-8c35-246adc3b21de"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'a': 3, 'b': 2})\n",
            "defaultdict(<class 'int'>, {('a', 1): 3, ('b', 1): 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#foreachPartition\n",
        "\n",
        "\"\"\"\n",
        "The foreachPartition() action applies a function to each partition of the RDD, rather than each element like foreach().\n",
        "\n",
        "It‚Äôs useful for efficient operations, such as opening a database connection once per partition instead of once per element.\n",
        "\n",
        "‚úÖ Syntax\n",
        "rdd.foreachPartition(func)\n",
        "func: A function that takes an iterator and processes all elements in a single partition.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6], 3)\n",
        "\n",
        "def process_partition(partition):\n",
        "  print(\"process new partition\")\n",
        "  for item in partition:\n",
        "    print(item)\n",
        "\n",
        "rdd.foreachPartition(process_partition)\n",
        "rdd.foreachPartition(lambda partition: print(list(partition)))\n"
      ],
      "metadata": {
        "id": "-yNnFY1vn1ZH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#treeAggregate\n",
        "\n",
        "\"\"\"\n",
        "treeAggregate() is an optimized version of the aggregate() action in Spark, used to aggregate data in parallel using a tree pattern,\n",
        "which reduces communication cost and improves performance.\n",
        "\n",
        "It is especially useful for large-scale aggregations, as it avoids bottlenecks that can occur when all partitions send their results directly\n",
        "to the driver.\n",
        "\n",
        "üßÆ Syntax\n",
        "rdd.treeAggregate(zeroValue)(seqOp, combOp, depth=2)\n",
        "zeroValue: Initial value used for both seqOp and combOp.\n",
        "seqOp: Function to merge an element from the RDD into the accumulator.\n",
        "combOp: Function to merge two accumulators.\n",
        "depth: Tree depth used to control the number of aggregation stages.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "# Use treeAggregate to compute (sum, count)\n",
        "zero_value = (0, 0)\n",
        "\n",
        "# seqOp: Add value to running total and count\n",
        "seq_op = lambda acc, x: (acc[0] + x, acc[1] + 1)\n",
        "\n",
        "# combOp: Merge two accumulators\n",
        "comb_op = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "\n",
        "result = rdd.treeAggregate(zero_value, seq_op, comb_op)\n",
        "\n",
        "print(f\"Sum: {result[0]}, Count: {result[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKjp9w7rW3J",
        "outputId": "ccafb580-56fd-4b6e-884d-bf3482963e41"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum: 15, Count: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TreeReduce\n",
        "\n",
        "treeReduce() is an optimized version of reduce() in Apache Spark that performs reduction in a tree-like hierarchical fashion, which can improve performance for large distributed datasets.\n",
        "\n",
        "| Feature     | `reduce()`               | `treeReduce()`                         |\n",
        "| ----------- | ------------------------ | -------------------------------------- |\n",
        "| Execution   | Linear aggregation       | Hierarchical (tree-based) aggregation  |\n",
        "| Performance | Slower on large clusters | Faster due to parallel reduction steps |\n",
        "| Usage       | Basic reductions         | Large-scale or expensive computations  |\n",
        "| Tuning      | No control over depth    | You can set the `depth` of the tree    |\n"
      ],
      "metadata": {
        "id": "P3LF8Q0JsNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "‚úÖ Syntax\n",
        "rdd.treeReduce(func, depth=2)\n",
        "func: A binary function used to reduce elements.\n",
        "depth: Optional. Controls the tree's depth (default is 2). More depth = more stages but less data per stage.\n",
        "üí° When to Use treeReduce()\n",
        "When working with large RDDs across many partitions.\n",
        "When a faster, more balanced reduction is needed.\n",
        "For costly aggregation functions where reducing communication overhead is critical.\n",
        "\n",
        "\"\"\"\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "print(rdd.treeReduce(lambda x,y: x+y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilOJj7WMsXmL",
        "outputId": "81c39696-c1b3-46aa-85e5-35615cd6a2a1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#countApprox\n",
        "\"Return approximate count of elements in the dataset, this method returns incomplete when execution time meets timeout.\"\n",
        "\n",
        "print(rdd.countApprox(timeout=10, confidence=0.95))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqp3ulhmtEx2",
        "outputId": "e711cfd2-b711-42f9-fcb4-1eb65983670a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#countapproxdistinct\n",
        "\"Return an approximate number of distinct elements in the dataset.\"\n",
        "print(rdd.countApproxDistinct(relativeSD=0.1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwKlL47ltTo1",
        "outputId": "7a153329-5d4e-4a9b-dfd0-78c4bfe007a8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd.first())\n",
        "print(rdd.min())\n",
        "print(rdd.max())\n",
        "print(rdd.top(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2yVM_axuUHQ",
        "outputId": "64641ec0-a891-486c-a903-44da9c24832e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "5\n",
            "[5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACTIONS:\n",
        "\n",
        "RDD actions are operations that return non-RDD values, since RDD‚Äôs are lazy they do not execute the transformation functions until we call actions. hence, all these functions trigger the transformations to execute and finally returns the value of the action functions to the driver program."
      ],
      "metadata": {
        "id": "nbkD--0Ju5jT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8clXYjI2vAGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj1AMXIEC2vY1k5pqerDrd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}